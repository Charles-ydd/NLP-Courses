{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# building the config\n",
    "config = BertConfig()\n",
    "\n",
    "model = BertModel(config=config)\n",
    "\n",
    "# save the model\n",
    "save_dir = \"/media/dyd/UDISK/output_model/bert-model/\"\n",
    "model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.7218,  1.1722, -1.2007,  ...,  0.5422,  0.9019,  0.0797],\n",
       "         [-0.6419,  0.1116, -1.0009,  ..., -0.3035,  0.0134,  0.4281],\n",
       "         [ 0.4921,  0.9032, -1.1902,  ...,  0.1121, -0.4422,  1.0324],\n",
       "         [ 0.6242,  1.0374, -1.3022,  ...,  0.2696,  0.2177, -0.2742]],\n",
       "\n",
       "        [[-0.0318,  0.9829, -1.5006,  ...,  0.8164,  0.1683,  0.1509],\n",
       "         [-0.5306,  1.5387, -1.6326,  ..., -0.3804, -0.4625,  0.3979],\n",
       "         [-0.1019, -0.0721,  0.6409,  ..., -0.1555, -1.1611,  0.0782],\n",
       "         [ 0.6899,  1.0638, -0.8494,  ...,  0.7175, -0.2739,  0.6161]],\n",
       "\n",
       "        [[-0.2035,  1.5311, -1.4224,  ...,  1.3440,  0.3439,  0.3108],\n",
       "         [-1.1245,  0.9387,  0.3165,  ..., -0.5737, -0.2346,  1.0194],\n",
       "         [ 0.4847,  0.8208, -0.5073,  ...,  0.2893, -0.4217,  0.7293],\n",
       "         [ 0.1394,  1.3052, -1.9946,  ...,  0.8204, -0.0102,  0.2817]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.4619, -0.0437,  0.4285,  ..., -0.1155,  0.2745, -0.2404],\n",
       "        [ 0.7145,  0.0720,  0.4190,  ...,  0.1940, -0.4462, -0.6616],\n",
       "        [ 0.6377, -0.3060,  0.5474,  ...,  0.1143,  0.1887, -0.7056]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n",
    "\n",
    "import torch\n",
    "\n",
    "#  model expects a batch of inputs, therefore, two dimention\n",
    "model_inputs = torch.tensor(encoded_sequences)\n",
    "\n",
    "output = model(model_inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 word based\n",
    "Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.<br>\n",
    "we need a custom token to represent words that are not in our vocabulary. This is known as the “unknown” token, often represented as ”[UNK]” or ””. It’s generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn’t able to retrieve a sensible representation of a word and you’re losing information along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jim', 'henson', 'was', 'a', 'puppeteer']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word based tokenizer\n",
    "tokenizer_text1 = \"jim henson was a puppeteer\".split()\n",
    "tokenizer_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 char based\n",
    "Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
    "1. The vocabulary is much smaller.\n",
    "2. There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 subword tokenization\n",
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
    "1. For example, tokenization will be split into \"token\" and \"ization\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 load and save\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyd/anaconda3/envs/vault/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2478, 1037, 10938, 2121, 2897, 2003, 3722, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer2 = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer2(\"Using a Transformer network is simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding \n",
    "# Translating text to numbers \n",
    "\n",
    "seq= \"using a transformer network is simple\"\n",
    "\n",
    "# use subword tokenization\n",
    "# WordPiece, BPE, Unigram method\n",
    "tokens = tokenizer2.tokenize(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2478, 1037, 10938, 2121, 2897, 2003, 3722]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer2.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'using a transformer network is simple'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding\n",
    "# from vocabulary indices, we want to get a string\n",
    "\n",
    "decoded_string = tokenizer2.decode(ids)\n",
    "decoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Handling multiple sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[  \u001b[39m101\u001b[39m,  \u001b[39m1045\u001b[39m,  \u001b[39m1005\u001b[39m,  \u001b[39m2310\u001b[39m,  \u001b[39m2042\u001b[39m,  \u001b[39m3403\u001b[39m,  \u001b[39m2005\u001b[39m,  \u001b[39m1037\u001b[39m, \u001b[39m17662\u001b[39m, \u001b[39m12172\u001b[39m,\n\u001b[1;32m      5\u001b[0m           \u001b[39m2607\u001b[39m,  \u001b[39m2026\u001b[39m,  \u001b[39m2878\u001b[39m,  \u001b[39m2166\u001b[39m,  \u001b[39m1012\u001b[39m,   \u001b[39m102\u001b[39m]])\n\u001b[0;32m----> 6\u001b[0m model(input1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# model expect a batch of inputs\n",
    "import torch\n",
    "\n",
    "input1 = torch.tensor([[101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
    "          2607,  2026,  2878,  2166,  1012,   102]])\n",
    "model(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[200, 200, 200], [200, 200, 0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding the inputs\n",
    "# add a special word called the padding token.\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer2.pad_token_id],\n",
    "]\n",
    "batched_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 attention masks\n",
    "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model). <br>\n",
    "[Example](https://huggingface.co/learn/nlp-course/chapter2/5?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m sequence2_ids \u001b[39m=\u001b[39m [[\u001b[39m200\u001b[39m, \u001b[39m200\u001b[39m]]\n\u001b[1;32m      5\u001b[0m batched_ids \u001b[39m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     [\u001b[39m200\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39m200\u001b[39m],\n\u001b[1;32m      7\u001b[0m     [\u001b[39m200\u001b[39m, \u001b[39m200\u001b[39m, tokenizer2\u001b[39m.\u001b[39mpad_token_id],\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m \u001b[39mprint\u001b[39m(model(torch\u001b[39m.\u001b[39;49mtensor(sequence1_ids))\u001b[39m.\u001b[39;49mlogits)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(model(torch\u001b[39m.\u001b[39mtensor(sequence2_ids))\u001b[39m.\u001b[39mlogits)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(model(torch\u001b[39m.\u001b[39mtensor(batched_ids))\u001b[39m.\u001b[39mlogits)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "# attention masks\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer2.pad_token_id],\n",
    "]\n",
    "\n",
    "\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Putting it all together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0500,  0.0015],\n",
       "        [-0.1866, -0.0362]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens =tokenizer3(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vault",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
