{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyd/anaconda3/envs/vault/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# building the config\n",
    "config = BertConfig()\n",
    "\n",
    "model = BertModel(config=config)\n",
    "\n",
    "# save the model\n",
    "save_dir = \"/media/dyd/UDISK/output_model/bert-model/\"\n",
    "model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.7218,  1.1722, -1.2007,  ...,  0.5422,  0.9019,  0.0797],\n",
       "         [-0.6419,  0.1116, -1.0009,  ..., -0.3035,  0.0134,  0.4281],\n",
       "         [ 0.4921,  0.9032, -1.1902,  ...,  0.1121, -0.4422,  1.0324],\n",
       "         [ 0.6242,  1.0374, -1.3022,  ...,  0.2696,  0.2177, -0.2742]],\n",
       "\n",
       "        [[-0.0318,  0.9829, -1.5006,  ...,  0.8164,  0.1683,  0.1509],\n",
       "         [-0.5306,  1.5387, -1.6326,  ..., -0.3804, -0.4625,  0.3979],\n",
       "         [-0.1019, -0.0721,  0.6409,  ..., -0.1555, -1.1611,  0.0782],\n",
       "         [ 0.6899,  1.0638, -0.8494,  ...,  0.7175, -0.2739,  0.6161]],\n",
       "\n",
       "        [[-0.2035,  1.5311, -1.4224,  ...,  1.3440,  0.3439,  0.3108],\n",
       "         [-1.1245,  0.9387,  0.3165,  ..., -0.5737, -0.2346,  1.0194],\n",
       "         [ 0.4847,  0.8208, -0.5073,  ...,  0.2893, -0.4217,  0.7293],\n",
       "         [ 0.1394,  1.3052, -1.9946,  ...,  0.8204, -0.0102,  0.2817]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.4619, -0.0437,  0.4285,  ..., -0.1155,  0.2745, -0.2404],\n",
       "        [ 0.7145,  0.0720,  0.4190,  ...,  0.1940, -0.4462, -0.6616],\n",
       "        [ 0.6377, -0.3060,  0.5474,  ...,  0.1143,  0.1887, -0.7056]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n",
    "\n",
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)\n",
    "\n",
    "output = model(model_inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 word based\n",
    "Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.<br>\n",
    "we need a custom token to represent words that are not in our vocabulary. This is known as the “unknown” token, often represented as ”[UNK]” or ””. It’s generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn’t able to retrieve a sensible representation of a word and you’re losing information along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jim', 'henson', 'was', 'a', 'puppeteer']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word based tokenizer\n",
    "tokenizer_text1 = \"jim henson was a puppeteer\".split()\n",
    "tokenizer_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 char based\n",
    "Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
    "1. The vocabulary is much smaller.\n",
    "2. There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 subword tokenization\n",
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
    "1. For example, tokenization will be split into \"token\" and \"ization\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 load and save\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2478, 1037, 10938, 2121, 2897, 2003, 3722, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer2 = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer2(\"Using a Transformer network is simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['using', 'a', 'transform', '##er', 'network', 'is', 'simple']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding \n",
    "# Translating text to numbers \n",
    "\n",
    "seq= \"using a transformer network is simple\"\n",
    "\n",
    "# use subword tokenization\n",
    "tokens = tokenizer2.tokenize(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2478, 1037, 10938, 2121, 2897, 2003, 3722]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer2.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'using a transformer network is simple'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding\n",
    "# from vocabulary indices, we want to get a string\n",
    "\n",
    "decoded_string = tokenizer2.decode(ids)\n",
    "decoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Handling multiple sequences\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vault",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
