{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have parameters in your model, which should be saved and restored in the state_dict(), but not trained by the optimizer, you should register them as buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.0579, -0.0224,  0.2381],\n",
      "          [ 0.1789,  0.2221,  0.1635],\n",
      "          [ 0.3322,  0.3205, -0.2696]]],\n",
      "\n",
      "\n",
      "        [[[-0.0981,  0.1579,  0.2701],\n",
      "          [ 0.0863,  0.1182,  0.2472],\n",
      "          [ 0.1960,  0.0325,  0.2782]]],\n",
      "\n",
      "\n",
      "        [[[-0.0234,  0.0636, -0.2284],\n",
      "          [-0.3239, -0.0875,  0.1926],\n",
      "          [ 0.2080,  0.1421, -0.0819]]],\n",
      "\n",
      "\n",
      "        [[[-0.1942,  0.2092, -0.1111],\n",
      "          [ 0.1764, -0.2321, -0.1233],\n",
      "          [ 0.0385,  0.0964,  0.2867]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3141,  0.2319, -0.3064],\n",
      "          [-0.1830,  0.1184,  0.2310],\n",
      "          [-0.2528, -0.2914,  0.0233]]],\n",
      "\n",
      "\n",
      "        [[[-0.0335, -0.1931, -0.1619],\n",
      "          [ 0.2001, -0.2876,  0.1103],\n",
      "          [ 0.0338, -0.2547,  0.3255]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([-0.0223,  0.1173,  0.0912, -0.0221,  0.2902,  0.0723], device='cuda:0',\n",
      "       requires_grad=True))]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "module = model.conv1\n",
    "print(list(module.named_parameters()))\n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_structured() Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by:\n",
    "* 1. adding a named buffer called \"name + _mask\" corresponding to the binary mask applied to the parameter \"name\" by the pruning method.\n",
    "* 2. replacing the parammeter \"name\" by its pruned version, while the original parameter is stored in a new parameter named \"name + _orig\".<br>\n",
    "\n",
    "Parameter:\n",
    "* module - module containing the tensor to prune.\n",
    "* name(str) - parameter name within module on which pruning will act.\n",
    "* amount(int or float) - quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([-0.0223,  0.1173,  0.0912, -0.0221,  0.2902,  0.0723], device='cuda:0',\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0579, -0.0224,  0.2381],\n",
      "          [ 0.1789,  0.2221,  0.1635],\n",
      "          [ 0.3322,  0.3205, -0.2696]]],\n",
      "\n",
      "\n",
      "        [[[-0.0981,  0.1579,  0.2701],\n",
      "          [ 0.0863,  0.1182,  0.2472],\n",
      "          [ 0.1960,  0.0325,  0.2782]]],\n",
      "\n",
      "\n",
      "        [[[-0.0234,  0.0636, -0.2284],\n",
      "          [-0.3239, -0.0875,  0.1926],\n",
      "          [ 0.2080,  0.1421, -0.0819]]],\n",
      "\n",
      "\n",
      "        [[[-0.1942,  0.2092, -0.1111],\n",
      "          [ 0.1764, -0.2321, -0.1233],\n",
      "          [ 0.0385,  0.0964,  0.2867]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3141,  0.2319, -0.3064],\n",
      "          [-0.1830,  0.1184,  0.2310],\n",
      "          [-0.2528, -0.2914,  0.0233]]],\n",
      "\n",
      "\n",
      "        [[[-0.0335, -0.1931, -0.1619],\n",
      "          [ 0.2001, -0.2876,  0.1103],\n",
      "          [ 0.0338, -0.2547,  0.3255]]]], device='cuda:0', requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)\n",
    "print(list(module.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]]], device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))\n",
    "# 0 is the pruned position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0579, -0.0224,  0.0000],\n",
       "          [ 0.1789,  0.0000,  0.1635],\n",
       "          [ 0.0000,  0.3205, -0.2696]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.1579,  0.0000],\n",
       "          [ 0.0863,  0.1182,  0.0000],\n",
       "          [ 0.0000,  0.0325,  0.2782]]],\n",
       "\n",
       "\n",
       "        [[[-0.0234,  0.0000, -0.2284],\n",
       "          [-0.3239, -0.0875,  0.1926],\n",
       "          [ 0.2080,  0.1421, -0.0819]]],\n",
       "\n",
       "\n",
       "        [[[-0.1942,  0.0000, -0.1111],\n",
       "          [ 0.1764, -0.2321, -0.0000],\n",
       "          [ 0.0000,  0.0964,  0.2867]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3141,  0.2319, -0.3064],\n",
       "          [-0.1830,  0.1184,  0.0000],\n",
       "          [-0.2528, -0.2914,  0.0233]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.1931, -0.1619],\n",
       "          [ 0.2001, -0.2876,  0.0000],\n",
       "          [ 0.0000, -0.2547,  0.0000]]]], device='cuda:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured object at 0x7fdb42caa650>)])\n"
     ]
    }
   ],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prune the bias\n",
    "prune.l1_unstructured(module, name=\"bias\", amount=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0579, -0.0224,  0.2381],\n",
      "          [ 0.1789,  0.2221,  0.1635],\n",
      "          [ 0.3322,  0.3205, -0.2696]]],\n",
      "\n",
      "\n",
      "        [[[-0.0981,  0.1579,  0.2701],\n",
      "          [ 0.0863,  0.1182,  0.2472],\n",
      "          [ 0.1960,  0.0325,  0.2782]]],\n",
      "\n",
      "\n",
      "        [[[-0.0234,  0.0636, -0.2284],\n",
      "          [-0.3239, -0.0875,  0.1926],\n",
      "          [ 0.2080,  0.1421, -0.0819]]],\n",
      "\n",
      "\n",
      "        [[[-0.1942,  0.2092, -0.1111],\n",
      "          [ 0.1764, -0.2321, -0.1233],\n",
      "          [ 0.0385,  0.0964,  0.2867]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3141,  0.2319, -0.3064],\n",
      "          [-0.1830,  0.1184,  0.2310],\n",
      "          [-0.2528, -0.2914,  0.0233]]],\n",
      "\n",
      "\n",
      "        [[[-0.0335, -0.1931, -0.1619],\n",
      "          [ 0.2001, -0.2876,  0.1103],\n",
      "          [ 0.0338, -0.2547,  0.3255]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([-0.0223,  0.1173,  0.0912, -0.0221,  0.2902,  0.0723], device='cuda:0',\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.ln_structured(module, name=\"weight\", amount=0.3, n=2, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding hook will now be of type torch.nn.utils.prune.PruningContainer, and will store the history of pruning applied to the weight parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.nn.utils.prune.RandomUnstructured object at 0x7fdb42caa650>, <torch.nn.utils.prune.LnStructured object at 0x7fdb42cab310>]\n"
     ]
    }
   ],
   "source": [
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == \"weight\":\n",
    "        break\n",
    "print(list(hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove \"weight_mask\" in named_buffers()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter 'weight' of module Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) has to be pruned before pruning can be removed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dyd/Python_Projects/Learning_NLP/pytorch_pruning_tutorial.ipynb 单元格 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dyd/Python_Projects/Learning_NLP/pytorch_pruning_tutorial.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prune\u001b[39m.\u001b[39;49mremove(module, \u001b[39m\"\u001b[39;49m\u001b[39mweight\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dyd/Python_Projects/Learning_NLP/pytorch_pruning_tutorial.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlist\u001b[39m(module\u001b[39m.\u001b[39mnamed_parameters()))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dyd/Python_Projects/Learning_NLP/pytorch_pruning_tutorial.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlist\u001b[39m(module\u001b[39m.\u001b[39mnamed_buffers()))\n",
      "File \u001b[0;32m~/anaconda3/envs/vault/lib/python3.10/site-packages/torch/nn/utils/prune.py:1197\u001b[0m, in \u001b[0;36mremove\u001b[0;34m(module, name)\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[39mdel\u001b[39;00m module\u001b[39m.\u001b[39m_forward_pre_hooks[k]\n\u001b[1;32m   1195\u001b[0m         \u001b[39mreturn\u001b[39;00m module\n\u001b[0;32m-> 1197\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1198\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m of module \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m has to be pruned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1199\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbefore pruning can be removed\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, module)\n\u001b[1;32m   1200\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter 'weight' of module Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) has to be pruned before pruning can be removed"
     ]
    }
   ],
   "source": [
    "prune.remove(module, \"weight\")\n",
    "print(list(module.named_parameters()))\n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])\n"
     ]
    }
   ],
   "source": [
    "new_model = LeNet()\n",
    "for name, module in new_model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)\n",
    "\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.4)\n",
    "\n",
    "\n",
    "print(dict(new_model.named_buffers()).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.conv1, 'weight'),\n",
    "    (model.conv2, 'weight'),\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    "    (model.fc3, 'weight'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor.nelement() : renturn the number of elements in tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 5.56%\n",
      "Sparsity in conv2.weight: 7.06%\n",
      "Sparsity in fc1.weight: 22.15%\n",
      "Sparsity in fc2.weight: 11.88%\n",
      "Sparsity in fc3.weight: 8.93%\n",
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.conv1.weight == 0))\n",
    "        / float(model.conv1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.conv2.weight == 0))\n",
    "        / float(model.conv2.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc1.weight == 0))\n",
    "        / float(model.fc1.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc2.weight == 0))\n",
    "        / float(model.fc2.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc3.weight == 0))\n",
    "        / float(model.fc3.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.conv1.weight == 0)\n",
    "            + torch.sum(model.conv2.weight == 0)\n",
    "            + torch.sum(model.fc1.weight == 0)\n",
    "            + torch.sum(model.fc2.weight == 0)\n",
    "            + torch.sum(model.fc3.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.conv1.weight.nelement()\n",
    "            + model.conv2.weight.nelement()\n",
    "            + model.fc1.weight.nelement()\n",
    "            + model.fc2.weight.nelement()\n",
    "            + model.fc3.weight.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom pruning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FooBarPruningMethod(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        mask.view(-1)[::2] = 0\n",
    "        return mask\n",
    "    \n",
    "def foobar_unstructured(module,name):\n",
    "    FooBarPruningMethod.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 0.],\n",
       "          [1., 0., 1.],\n",
       "          [0., 1., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 1.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[0., 1., 0.],\n",
       "          [1., 0., 1.],\n",
       "          [0., 1., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 1.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[0., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [0., 1., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 1.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 1.]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_mask(default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        mask.view(-1)[::2] = 0\n",
    "        return mask\n",
    "\n",
    "compute_mask(model.conv1.weight_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the source code of apply() method, \n",
    "```\n",
    "if not isinstance(method, PruningContainer):\n",
    "    # copy `module[name]` to `module[name + '_orig']`\n",
    "    module.register_parameter(name + \"_orig\", orig)\n",
    "    # temporarily delete `module[name]`\n",
    "    del module._parameters[name]\n",
    "    default_mask = torch.ones_like(orig)  # temp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "foobar_unstructured(model.fc3, name='bias')\n",
    "\n",
    "print(model.fc3.bias_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vault",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
